{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "columns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"level\"]\n",
    "attack_type = {\n",
    "    0: 'normal',\n",
    "    1: 'DOS',\n",
    "    2: 'Probe',\n",
    "    3: 'R2L',\n",
    "    4: 'U2R'\n",
    "}\n",
    "\n",
    "new_label = {\n",
    "    'normal': 0,\n",
    "    'apache2': 1,\n",
    "    'back': 1,\n",
    "    'mailbomb': 1,\n",
    "    'processtable': 1,\n",
    "    'snmpgetattack': 1,\n",
    "    'teardrop': 1,\n",
    "    'smurf': 1,\n",
    "    'land': 1,\n",
    "    'neptune': 1,\n",
    "    'pod': 1,\n",
    "    'udpstorm': 1,\n",
    "    'nmap': 2,\n",
    "    'ipsweep': 2,\n",
    "    'portsweep': 2,\n",
    "    'satan': 2,\n",
    "    'mscan': 2,\n",
    "    'saint': 2,\n",
    "    'ftp_write': 3,\n",
    "    'guess_passwd': 3,\n",
    "    'snmpguess': 3,\n",
    "    'imap': 3,\n",
    "    'spy': 3,\n",
    "    'warezclient': 3,\n",
    "    'warezmaster': 3,\n",
    "    'multihop': 3,\n",
    "    'phf': 3,\n",
    "    'imap': 3,\n",
    "    'named': 3,\n",
    "    'sendmail': 3,\n",
    "    'xlock': 3,\n",
    "    'xsnoop': 3,\n",
    "    'worm': 3,\n",
    "    'ps': 4,\n",
    "    'buffer_overflow': 4,\n",
    "    'perl': 4,\n",
    "    'rootkit': 4,\n",
    "    'loadmodule': 4,\n",
    "    'xterm': 4,\n",
    "    'sqlattack': 4,\n",
    "    'httptunnel': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop na and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"NSL-KDD/KDDTrain+.txt\", header=None)\n",
    "train.columns = columns\n",
    "train = train.dropna()\n",
    "train = train.drop_duplicates()\n",
    "test = pd.read_csv(\"NSL-KDD/KDDTest+.txt\", header=None)\n",
    "test.columns = columns\n",
    "test = test.dropna()\n",
    "test = test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index_train = train.index.to_list()[-1]\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoder at `protocol_type`, `service` and `flag` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import encode_text_dummy\n",
    "encode_text_dummy(df=df, name=\"protocol_type\")\n",
    "encode_text_dummy(df=df, name=\"service\")\n",
    "encode_text_dummy(df=df, name=\"flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[:last_index_train+1]\n",
    "test = df.iloc[last_index_train+1:]\n",
    "X_train = train.drop(columns=['label', 'level'])\n",
    "y_train = train.label.map(new_label).map(attack_type)\n",
    "X_test = test.drop(columns=['label', 'level'])\n",
    "y_test = test.label.map(new_label).map(attack_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize trainset/testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "features = X_train.columns\n",
    "scalser = MinMaxScaler()\n",
    "scalser.fit(X_train)\n",
    "X_train = scalser.transform(X_train)\n",
    "X_test = scalser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE for balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "desired_samples = {\n",
    "    \"U2R\": 5000,\n",
    "    \"R2L\": 5000\n",
    "}\n",
    "\n",
    "bal = SMOTE(sampling_strategy=desired_samples)\n",
    "\n",
    "X_train, y_train = bal.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP NEURAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=features)\n",
    "X_test = pd.DataFrame(X_test, columns=features)\n",
    "y_train = pd.DataFrame(y_train, columns=['label'])\n",
    "y_test = pd.DataFrame(y_test, columns=['label'])\n",
    "encode_text_dummy(y_train, 'label')\n",
    "encode_text_dummy(y_test, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"NSL-KDD/X_train.csv\")\n",
    "y_train.to_csv(\"NSL-KDD/y_train.csv\")\n",
    "X_test.to_csv(\"NSL-KDD/X_test.csv\")\n",
    "y_test.to_csv(\"NSL-KDD/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from art.estimators.tensorflow import TensorFlowV2Estimator\n",
    "\n",
    "layers = [\n",
    "    tf.keras.layers.Dense(X_train.shape[0], activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "]\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dnn = tf.keras.models.Sequential(layers=layers)\n",
    "dnn.compile(\n",
    "    loss=loss_object,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "    ]\n",
    ")\n",
    "dnn.fit(\n",
    "    x=X_train,\n",
    "    y=y_train.values,\n",
    "    epochs=5,\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.save(\"NSL-KDD/dnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP and ART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model, train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dnn = tf.keras.models.load_model(\"NSL-KDD/dnn.h5\")\n",
    "X_train = pd.read_csv(\"NSL-KDD/X_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_train = pd.read_csv(\"NSL-KDD/y_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "X_test = pd.read_csv(\"NSL-KDD/X_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_test = pd.read_csv(\"NSL-KDD/y_test.csv\").drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dnn_model = TensorFlowV2Classifier(\n",
    "    model=dnn,\n",
    "    loss_object=loss_object,\n",
    "    optimizer=optimizer,\n",
    "    nb_classes=5,\n",
    "    input_shape=X_train.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_metric import evaluate_metric\n",
    "evaluate_metric(y_pred=dnn_model.predict(x=X_test), y_true=y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = X_test.sample(n=500)\n",
    "y_samples = y_test.iloc[samples.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_metric import evaluate_metric\n",
    "evaluate_metric(\n",
    "    y_pred=dnn_model.predict(x=samples),\n",
    "    y_true=y_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate shap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap_importance import shap_importance\n",
    "\n",
    "background = pd.read_csv(\"NSL-KDD/samples/background.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "samples = pd.read_csv(\"NSL-KDD/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "y_samples = pd.read_csv(\"NSL-KDD/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "explainer = shap.DeepExplainer(model=dnn, data=background.values)\n",
    "shap_vals = explainer.shap_values(samples.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.summary_plot(shap_vals, samples.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy plot when increase number of adversarial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from adversarial_n_best_worst_features import adversarial_n_best_worst_features\n",
    "from shap_importance import shap_importance\n",
    "from art.attacks.evasion.fast_gradient import FastGradientMethod\n",
    "from art.attacks.evasion.iterative_method import BasicIterativeMethod\n",
    "from art.attacks.evasion.saliency_map import SaliencyMapMethod\n",
    "from art.attacks.evasion.carlini import CarliniL0Method, CarliniLInfMethod ,CarliniL2Method\n",
    "from art.attacks.evasion.deepfool import DeepFool\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "\n",
    "dnn = tf.keras.models.load_model(\"NSL-KDD/dnn.h5\")\n",
    "X_train = pd.read_csv(\"NSL-KDD/X_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_train = pd.read_csv(\"NSL-KDD/y_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "X_test = pd.read_csv(\"NSL-KDD/X_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_test = pd.read_csv(\"NSL-KDD/y_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dnn_model = TensorFlowV2Classifier(\n",
    "    model=dnn,\n",
    "    loss_object=loss_object,\n",
    "    optimizer=optimizer,\n",
    "    nb_classes=5,\n",
    "    input_shape=X_train.shape\n",
    ")\n",
    "\n",
    "\n",
    "# background = pd.read_csv(\"NSL-KDD/samples/background.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "samples = X_train.sample(n=10000).copy()#pd.read_csv(\"NSL-KDD/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "y_samples = y_train.iloc[samples.index].copy()#pd.read_csv(\"NSL-KDD/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "shap_vals = pickle.load(open(\"NSL-KDD/samples/shap_vals_of_samples.pkl\",\"rb\"))\n",
    "shap_vals = [np.array(val) for val in shap_vals]\n",
    "\n",
    "adversarial_algs = {\n",
    "    \"FSGM\": FastGradientMethod(estimator=dnn_model, eps=0.2),\n",
    "    \"BIM\": BasicIterativeMethod(estimator=dnn_model, eps=0.2, max_iter=100, batch_size=32),\n",
    "    # \"CW-L2\": CarliniL2Method(classifier=dnn_model, max_iter=10),\n",
    "    \"JSMA\": SaliencyMapMethod(classifier=dnn_model,theta=0.1,gamma=1, batch_size=1),\n",
    "    \"DeepFool\": DeepFool(classifier=dnn_model, max_iter=100, epsilon=0.000001, nb_grads=10, batch_size=1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features = samples.columns\n",
    "shap_importance_df = shap_importance(full_features, shap_val_of_sample=shap_vals)\n",
    "selected_features = [f for f in shap_importance_df.loc[shap_importance_df['shap_importance'] > 0]['column_name'].values]\n",
    "selected_features = [i for i in selected_features if 'protocol_type' not in i and 'service' not in i and 'flag' not in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate AE sample with popular algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as plk\n",
    "acc_list = []\n",
    "\n",
    "for alg_name in adversarial_algs:\n",
    "    print(f\"Start {alg_name}\")\n",
    "    alg = adversarial_algs[alg_name]\n",
    "    org_samples = samples.copy()\n",
    "    adv_samples = alg.generate(x=org_samples.values)\n",
    "    adv_samples = pd.DataFrame(adv_samples, columns=full_features)\n",
    "    print(f\"\\t{alg_name} finish generate adversarial.\")\n",
    "    pd.DataFrame(np.clip(adv_samples,0,1), columns=full_features).to_csv(f\"NSL-KDD/samples/attack/X_{alg_name}_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast Gradient Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_on_fsgm = adversarial_n_best_worst_features(\n",
    "    model=dnn_model,\n",
    "    selected_features=selected_features,\n",
    "    samples=pd.read_csv(\"NSL-KDD/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"]),\n",
    "    adv_samples=pd.read_csv(\"NSL-KDD/samples/attack/FSGM_sample.csv\").drop(columns=[\"Unnamed: 0\"]),\n",
    "    y_true=pd.read_csv(\"NSL-KDD/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    ")\n",
    "\n",
    "acc_on_bim = adversarial_n_best_worst_features(\n",
    "    model=dnn_model,\n",
    "    selected_features=selected_features,\n",
    "    samples=pd.read_csv(\"NSL-KDD/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"]),\n",
    "    adv_samples=pd.read_csv(\"NSL-KDD/samples/attack/BIM_sample.csv\"),\n",
    "    y_true=pd.read_csv(\"NSL-KDD/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    ")\n",
    "\n",
    "acc_on_jsma = adversarial_n_best_worst_features(\n",
    "    model=dnn_model,\n",
    "    selected_features=selected_features,\n",
    "    samples=pd.read_csv(\"NSL-KDD/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"]),\n",
    "    adv_samples=pd.read_csv(\"NSL-KDD/samples/attack/JSMA_sample.csv\"),\n",
    "    y_true=pd.read_csv(\"NSL-KDD/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    ")\n",
    "\n",
    "acc_on_deepfool = adversarial_n_best_worst_features(\n",
    "    model=dnn_model,\n",
    "    selected_features=selected_features,\n",
    "    samples=pd.read_csv(\"NSL-KDD/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"]),\n",
    "    adv_samples=pd.read_csv(\"NSL-KDD/samples/attack/DeepFool_sample.csv\"),\n",
    "    y_true=pd.read_csv(\"NSL-KDD/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
