{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"UNSW-NB15/UNSW_NB15_training-set.csv\")\n",
    "test = pd.read_csv(\"UNSW-NB15/UNSW_NB15_testing-set.csv\")\n",
    "\n",
    "df = pd.concat([train, test])\n",
    "df = df.drop(columns=['id'])\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import encode_text_dummy\n",
    "\n",
    "encode_text_dummy(df, \"proto\")\n",
    "encode_text_dummy(df, \"service\")\n",
    "encode_text_dummy(df, \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['attack_cat', 'label'])\n",
    "y = df[['attack_cat']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    random_state=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "balance = SMOTE(\n",
    "    sampling_strategy={\n",
    "        \"Reconnaissance\":10000,\n",
    "        \"Generic\": 10000,\n",
    "        \"DoS\":10000,\n",
    "        \"Analysis\":10000,\n",
    "        \"Backdoor\":10000,\n",
    "        \"Shellcode\":10000,\n",
    "        \"Worms\":10000,\n",
    "        }\n",
    "    )\n",
    "X_train, y_train = balance.fit_resample(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seem balance, so I don't need to use SMOTE to balance train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=columns)\n",
    "X_test = pd.DataFrame(X_test, columns=columns)\n",
    "encode_text_dummy(y_train, \"attack_cat\")\n",
    "encode_text_dummy(y_test, \"attack_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"UNSW-NB15/X_train.csv\")\n",
    "X_test.to_csv(\"UNSW-NB15/X_test.csv\")\n",
    "y_train.to_csv(\"UNSW-NB15/y_train.csv\")\n",
    "y_test.to_csv(\"UNSW-NB15/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP NEURAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = pd.read_csv(\"UNSW-NB15/X_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "X_test = pd.read_csv(\"UNSW-NB15/X_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_train = pd.read_csv(\"UNSW-NB15/y_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_test = pd.read_csv(\"UNSW-NB15/y_test.csv\").drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(y_train.columns)\n",
    "layers = [\n",
    "  tf.keras.layers.Dense(X_train.shape[0], activation='relu', input_shape=(X_train.shape[1],)),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "]\n",
    "dnn = tf.keras.Sequential(layers)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "dnn.compile(\n",
    "  optimizer=optimizer,\n",
    "  loss=loss_object,\n",
    "  metrics=[\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall()\n",
    "  ]\n",
    ")\n",
    "\n",
    "dnn.fit(X_train, y_train.values, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.save(\"UNSW-NB15/dnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120/5120 [==============================] - 152s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion_matrix': array([[ 7630,  2190,    79,    16,     5,     0,    73,     7,     0,\n",
       "             0],\n",
       "        [ 1868,  7876,    88,    31,     0,     1,    10,    41,    75,\n",
       "            10],\n",
       "        [ 1215,  1420,  5197,  1377,    77,    60,   141,   147,   308,\n",
       "            58],\n",
       "        [  563,   619,   835, 15168,   211,   151,   489,   686,   390,\n",
       "           100],\n",
       "        [  516,   573,   122,   133,  9038,    17,  3891,   137,   231,\n",
       "            10],\n",
       "        [  133,   151,   134,   185,    38,  9263,    36,     9,    41,\n",
       "            10],\n",
       "        [  312,    10,   103,   234,  2804,    15, 56122,   107,   246,\n",
       "             7],\n",
       "        [  694,   646,   135,   401,    31,     4,    53,  7914,   118,\n",
       "             4],\n",
       "        [    0,     5,     2,    12,    60,    18,    46,   172,  9679,\n",
       "             6],\n",
       "        [    0,     0,     0,     6,     0,     0,     0,     1,   117,\n",
       "          9876]]),\n",
       " 'accuracy_score': 0.840838623046875,\n",
       " 'precision_score': 0.840838623046875,\n",
       " 'recall_score': 0.840838623046875,\n",
       " 'f1_score': 0.840838623046875}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation_metric import evaluate_metric\n",
    "evaluate_metric(\n",
    "    y_pred=dnn.predict(X_train),\n",
    "    y_true=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1526/1526 [==============================] - 47s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion_matrix': array([[  154,   269,    69,    14,    11,     0,    65,    25,     0,\n",
       "             0],\n",
       "        [  246,   136,    66,    22,    15,     7,     3,    28,     9,\n",
       "             0],\n",
       "        [  258,   252,   416,   462,    49,    45,    38,    40,    53,\n",
       "             9],\n",
       "        [  261,   298,   574,  5942,   135,   143,   251,   348,   187,\n",
       "            83],\n",
       "        [  248,   245,    91,   103,  3481,    15,  1926,    47,   127,\n",
       "             9],\n",
       "        [   25,    43,    64,   150,    13,  1907,     9,     3,    12,\n",
       "             6],\n",
       "        [  189,     9,    60,   161,  1413,    16, 23706,    56,   147,\n",
       "             5],\n",
       "        [  199,   222,    52,   197,    17,     2,    28,  2278,    69,\n",
       "            12],\n",
       "        [    0,     0,    11,    23,    20,     5,    23,    18,   328,\n",
       "             3],\n",
       "        [    0,     0,     3,    21,     1,     0,     0,     0,     4,\n",
       "            19]]),\n",
       " 'accuracy_score': 0.7858225462887105,\n",
       " 'precision_score': 0.7858225462887105,\n",
       " 'recall_score': 0.7858225462887105,\n",
       " 'f1_score': 0.7858225462887105}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation_metric import evaluate_metric\n",
    "evaluate_metric(\n",
    "    y_pred=dnn.predict(X_test),\n",
    "    y_true=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "\n",
    "X_train = pd.read_csv(\"UNSW-NB15/X_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "X_test = pd.read_csv(\"UNSW-NB15/X_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_train = pd.read_csv(\"UNSW-NB15/y_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_test = pd.read_csv(\"UNSW-NB15/y_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "\n",
    "dnn = tf.keras.models.load_model(\"UNSW-NB15/dnn.h5\")\n",
    "dnn_model = TensorFlowV2Classifier(\n",
    "    model=dnn,\n",
    "    optimizer=optimizer,\n",
    "    loss_object=loss_object,\n",
    "    input_shape=X_train.shape,\n",
    "    nb_classes=len(y_train.columns)\n",
    ")\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate shap value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "background = pd.read_csv(\"UNSW-NB15/samples/background.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "samples = pd.read_csv(\"UNSW-NB15/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "y_samples = pd.read_csv(\"UNSW-NB15/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "explainer = shap.DeepExplainer(model=dnn, data=background.values)\n",
    "shap_vals = explainer.shap_values(samples.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_shap_vals = [list(val) for val in shap_vals]\n",
    "import pickle as pkl\n",
    "pkl.dump(\n",
    "    the_shap_vals,\n",
    "    open(\"UNSW-NB15/samples/shap_vals_of_sample.pkl\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate AE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from adversarial_n_best_worst_features import adversarial_n_best_worst_features\n",
    "from shap_importance import shap_importance\n",
    "from art.attacks.evasion.fast_gradient import FastGradientMethod\n",
    "from art.attacks.evasion.iterative_method import BasicIterativeMethod\n",
    "from art.attacks.evasion.saliency_map import SaliencyMapMethod\n",
    "from art.attacks.evasion.carlini import CarliniL0Method, CarliniLInfMethod ,CarliniL2Method\n",
    "from art.attacks.evasion.deepfool import DeepFool\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "\n",
    "dnn = tf.keras.models.load_model(\"UNSW-NB15/dnn.h5\")\n",
    "X_train = pd.read_csv(\"UNSW-NB15/X_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_train = pd.read_csv(\"UNSW-NB15/y_train.csv\").drop(columns=['Unnamed: 0'])\n",
    "X_test = pd.read_csv(\"UNSW-NB15/X_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "y_test = pd.read_csv(\"UNSW-NB15/y_test.csv\").drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dnn_model = TensorFlowV2Classifier(\n",
    "    model=dnn,\n",
    "    loss_object=loss_object,\n",
    "    optimizer=optimizer,\n",
    "    nb_classes=5,\n",
    "    input_shape=X_train.shape\n",
    ")\n",
    "\n",
    "\n",
    "# background = pd.read_csv(\"UNSW-NB15/samples/background.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "samples = pd.read_csv(\"UNSW-NB15/samples/samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "y_samples = pd.read_csv(\"UNSW-NB15/samples/y_samples.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "shap_vals = pickle.load(open(\"UNSW-NB15/samples/shap_vals_of_samples.pkl\",\"rb\"))\n",
    "shap_vals = [np.array(val) for val in shap_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_algs = {\n",
    "    # \"FSGM\": FastGradientMethod(estimator=dnn_model, eps=0.2),\n",
    "    # \"BIM\": BasicIterativeMethod(estimator=dnn_model, eps=0.2, max_iter=100, batch_size=32),\n",
    "    # \"CW-L2\": CarliniL2Method(classifier=dnn_model, max_iter=10),\n",
    "    \"JSMA\": SaliencyMapMethod(classifier=dnn_model,theta=0.1,gamma=1, batch_size=1),\n",
    "    \"DeepFool\": DeepFool(classifier=dnn_model, max_iter=100, epsilon=0.000001, nb_grads=10, batch_size=1),\n",
    "}\n",
    "\n",
    "# \n",
    "full_features = samples.columns\n",
    "selected_features = [f for f in shap_importance(full_features, shap_val_of_sample=shap_vals).column_name.values.tolist() \\\n",
    "                     if 'proto' not in f and 'service' not in f and 'state' not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as plk\n",
    "acc_list = []\n",
    "\n",
    "for alg_name in adversarial_algs:\n",
    "    print(f\"{alg_name}\")\n",
    "    alg = adversarial_algs[alg_name]\n",
    "    org_samples = samples.copy()\n",
    "    adv_samples = alg.generate(x=org_samples.values) + np.random.uniform(0,10**-10, size=org_samples.shape)\n",
    "    adv_samples = pd.DataFrame(adv_samples, columns=full_features)\n",
    "    print(f\"\\t{alg_name} finish generate adversarial.\")\n",
    "    pd.DataFrame(np.clip(adv_samples,0,1), columns=full_features).to_csv(f\"UNSW-NB15/samples/attack/{alg_name}_sample.csv\")\n",
    "    \n",
    "    acc_list.append(adversarial_n_best_worst_features(\n",
    "        model=dnn_model,\n",
    "        selected_features=selected_features,\n",
    "        samples=org_samples,\n",
    "        adv_samples=adv_samples,\n",
    "        y_true=y_samples\n",
    "    ))\n",
    "    \n",
    "    plk.dump(acc_list[-1], open(f'UNSW-NB15/samples/acc_attack/acc_{alg_name}.plk',\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
